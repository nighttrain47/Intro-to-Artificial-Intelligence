import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
from sklearn.naive_bayes import BernoulliNB, MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import confusion_matrix, roc_curve, auc

# --- Page Configuration ---
st.set_page_config(page_title="Naive Bayes Text Classification", layout="wide")
st.title("Naive Bayes Text Classification")
st.markdown("This app demonstrates text classification, enhanced to better understand the context of educational terminology.")

# --- Helper Functions ---

def engineer_features_for_text(text):
    """
    Performs feature engineering by adding a unique marker for text containing
    specific educational keywords that should be treated as positive.
    """
    # These words, while positive in an educational context, can be misclassified.
    educational_keywords = [
        "critical thinking", "critical skills", "critical analysis",
        "problem solving", "analytical skills", "complex world",
        "challenge", "challenging curriculum"
    ]
    
    text_lower = text.lower()
    for phrase in educational_keywords:
        if phrase in text_lower:
            # Add a unique, powerful marker for the model to learn from.
            return text + " _educational_positive_"
            
    return text

@st.cache_data
def split_train_test(data, ratio_test, random_seed=0):
    """Splits a DataFrame into training and testing sets."""
    np.random.seed(random_seed)
    shuffled_indices = np.random.permutation(len(data))
    test_size = int(len(data) * ratio_test)
    test_indices = shuffled_indices[:test_size]
    train_indices = shuffled_indices[test_size:]
    return data.iloc[train_indices], data.iloc[test_indices]

def preprocess_labels(labels):
    """Standardizes labels to a 0/1 integer format for consistency."""
    label_map = {
        'positive': 1, 'pos': 1, '1': 1, 1: 1,
        'negative': 0, 'neg': 0, '0': 0, 0: 0
    }
    
    if pd.api.types.is_string_dtype(labels):
        processed_labels = labels.str.lower().map(label_map)
    else:
        processed_labels = labels.map(label_map)

    if processed_labels.isnull().any():
        unrecognized = labels[processed_labels.isnull()].unique()
        st.warning(f"Unrecognized labels found and treated as 'negative': {unrecognized}. Ensure labels are standard for best results.")
        processed_labels = processed_labels.fillna(0)

    return processed_labels.astype(int)

def train_and_evaluate(X_train, y_train, X_test, model_type="bernoulli", binary_count=True):
    """Trains a Naive Bayes model and returns results, applying feature engineering first."""
    # Apply feature engineering to both training and test sets
    X_train_engineered = X_train.astype(str).fillna('').apply(engineer_features_for_text)
    X_test_engineered = X_test.astype(str).fillna('').apply(engineer_features_for_text)

    if X_train_engineered.str.strip().eq('').all():
        st.error("Error: The selected text column is empty after processing.")
        return None

    try:
        count_vect = CountVectorizer(binary=binary_count, stop_words='english')
        X_train_transformed = count_vect.fit_transform(X_train_engineered)
        X_test_transformed = count_vect.transform(X_test_engineered)

        if X_train_transformed.shape[1] == 0:
            st.error("No features were extracted from the text. Check data and stop words.")
            return None

        model = BernoulliNB() if model_type == "bernoulli" else MultinomialNB()
        model.fit(X_train_transformed, y_train)

        y_pred = model.predict(X_test_transformed)
        y_pred_proba = model.predict_proba(X_test_transformed)

        feature_names = count_vect.get_feature_names_out()
        feature_importance = {}
        if len(model.classes_) == 2:
            # Assuming classes are [0, 1] after preprocessing
            neg_idx, pos_idx = 0, 1 
            for class_idx, class_name in [(neg_idx, "negative"), (pos_idx, "positive")]:
                log_probs = model.feature_log_prob_[class_idx]
                sorted_indices = np.argsort(log_probs)[::-1]
                feature_importance[class_name] = [(feature_names[i], log_probs[i]) for i in sorted_indices[:15]]

        return {
            "model": model, "vectorizer": count_vect, "predictions": y_pred,
            "probabilities": y_pred_proba, "features": feature_names,
            "top_features": feature_importance
        }
    except Exception as e:
        st.error(f"An error occurred during model training: {e}")
        return None

def calculate_metrics(y_true, y_pred):
    """Calculates classification metrics for binary classification."""
    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])
    TN, FP, FN, TP = cm.ravel()

    accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0
    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    return {"confusion_matrix": cm, "accuracy": accuracy, "precision": precision, "recall": recall, "f1": f1}

def plot_roc_curve(y_test, y_pred_proba):
    """Plots the ROC curve for binary classification."""
    if y_pred_proba.shape[1] == 2:
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1], pos_label=1)
        roc_auc = auc(fpr, tpr)

        fig, ax = plt.subplots(figsize=(8, 6))
        ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
        ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        ax.set_xlim([0.0, 1.0]); ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('False Positive Rate'); ax.set_ylabel('True Positive Rate')
        ax.set_title('Receiver Operating Characteristic (ROC) Curve'); ax.legend(loc="lower right")
        return fig, roc_auc
    return None, None

def display_custom_prediction(model_name, results, user_text, show_analysis=True):
    """Preprocesses text, gets a prediction, and displays the results."""
    st.subheader(f"{model_name} Prediction")

    processed_text = engineer_features_for_text(user_text)
    
    user_vector = results['vectorizer'].transform([processed_text])
    user_pred = results['model'].predict(user_vector)[0]
    user_proba = results['model'].predict_proba(user_vector)[0]

    neg_prob, pos_prob = user_proba[0], user_proba[1]

    if user_pred == 1:
        pred_label, pred_prob, pred_color = "Positive", pos_prob, "green"
    else:
        pred_label, pred_prob, pred_color = "Negative", neg_prob, "red"
    
    confidence_msg = "‚úì High confidence" if pred_prob >= 0.6 else "‚ö†Ô∏è Low confidence"

    st.markdown(f"<h3 style='color:{pred_color};'>Prediction: {pred_label}</h3>", unsafe_allow_html=True)
    st.write(f"Confidence: {pred_prob:.2%} ({confidence_msg})")

    # Be transparent about feature engineering
    if processed_text != user_text:
        st.info("üìö Educational keywords detected. A special feature was added to guide the model.")
        with st.expander("See preprocessing details"):
            st.write("**Original Text:**")
            st.code(user_text, language=None)
            st.write("**Text After Feature Engineering (used by model):**")
            st.code(processed_text, language=None)

    probs_df = pd.DataFrame({'Class': ['Negative', 'Positive'], 'Probability': [neg_prob, pos_prob]})
    st.bar_chart(probs_df.set_index('Class'))

    if show_analysis:
        st.subheader("Influential Words in Your Text")
        pos_features = {word for word, score in results['top_features']['positive']}
        neg_features = {word for word, score in results['top_features']['negative']}
        
        html_parts = []
        for word in user_text.split():
            clean_word = re.sub(r'[^\w]', '', word).lower()
            if clean_word in pos_features:
                html_parts.append(f'<span style="background-color:#d4edda; color:#155724; padding: 2px 5px; border-radius: 3px;">{word}</span>')
            elif clean_word in neg_features:
                html_parts.append(f'<span style="background-color:#f8d7da; color:#721c24; padding: 2px 5px; border-radius: 3px;">{word}</span>')
            else:
                html_parts.append(word)
        
        st.markdown(f"<div style='line-height: 2;'>{' '.join(html_parts)}</div>", unsafe_allow_html=True)
        st.markdown("""
        **Legend:** <span style="background-color:#d4edda; color:#155724; padding:2px;">Positive word</span> | <span style="background-color:#f8d7da; color:#721c24; padding:2px;">Negative word</span>
        """, unsafe_allow_html=True)
        
    return pred_label, pred_prob

# --- Main App ---

def main():
    try:
        with st.sidebar:
            st.header("Settings")
            with st.expander("1. Data Options", expanded=True):
                uploaded_file = st.file_uploader("Upload CSV (or use default)", type=["csv"])
                data = pd.read_csv(uploaded_file) if uploaded_file else pd.read_csv('Education.csv')
                st.success(f"Loaded dataset with {len(data)} rows.")
                text_col = st.selectbox("Select text column:", data.columns, index=0)
                label_col = st.selectbox("Select label column:", data.columns, index=1)
            
            with st.expander("2. Model Parameters", expanded=True):
                test_ratio = st.slider("Test set ratio:", 0.1, 0.5, 0.2, 0.05)
                random_seed = st.number_input("Random seed:", 0, 100, 0)
                dual_model = st.checkbox("Compare Bernoulli vs. Multinomial", value=True)
                if not dual_model:
                    model_type = st.selectbox("Model type:", ["bernoulli", "multinomial"])

            if st.button("Train Model(s)", use_container_width=True):
                st.session_state.clear() # Reset state on new training
                with st.spinner("Processing data and training..."):
                    train_set, test_set = split_train_test(data, test_ratio, random_seed)
                    X_train, y_train_orig = train_set[text_col], train_set[label_col]
                    X_test, y_test_orig = test_set[text_col], test_set[label_col]
                    
                    y_train = preprocess_labels(y_train_orig)
                    y_test = preprocess_labels(y_test_orig)

                    st.session_state.update({
                        'y_test': y_test, 'text_col': text_col, 'label_col': label_col
                    })

                    if dual_model:
                        results = {
                            "bernoulli": train_and_evaluate(X_train, y_train, X_test, "bernoulli", True),
                            "multinomial": train_and_evaluate(X_train, y_train, X_test, "multinomial", False)
                        }
                        if all(results.values()):
                            st.session_state.results = results
                            st.session_state.metrics = {k: calculate_metrics(y_test, v["predictions"]) for k, v in results.items()}
                            st.session_state.model_trained = 'dual'
                    else:
                        results = train_and_evaluate(X_train, y_train, X_test, model_type, model_type == "bernoulli")
                        if results:
                            st.session_state.results = {"single": results}
                            st.session_state.metrics = {"single": calculate_metrics(y_test, results["predictions"])}
                            st.session_state.model_trained = 'single'
                
                if 'model_trained' in st.session_state:
                    st.success("Model training complete!")
                else:
                    st.error("Model training failed. Please check the console for errors.")

        # --- Display Results ---
        if 'model_trained' not in st.session_state:
            st.info("‚¨ÖÔ∏è Configure your settings in the sidebar and click 'Train Model(s)' to begin.")
            return

        user_text = st.text_area(
            "Test with Custom Text",
            "Education that fosters critical thinking is essential for navigating a complex world.",
            height=100
        )

        if st.session_state.model_trained == 'dual':
            st.header("Model Comparison")
            col1, col2 = st.columns(2)
            with col1:
                b_label, b_prob = display_custom_prediction("Bernoulli", st.session_state.results['bernoulli'], user_text, show_analysis=False)
            with col2:
                m_label, m_prob = display_custom_prediction("Multinomial", st.session_state.results['multinomial'], user_text, show_analysis=False)
            
            st.subheader("Performance Metrics")
            comp_data = {
                "Metric": ["Accuracy", "Precision", "Recall", "F1 Score"],
                "Bernoulli NB": [f"{st.session_state.metrics['bernoulli'][m]:.4f}" for m in ["accuracy", "precision", "recall", "f1"]],
                "Multinomial NB": [f"{st.session_state.metrics['multinomial'][m]:.4f}" for m in ["accuracy", "precision", "recall", "f1"]]
            }
            st.table(pd.DataFrame(comp_data).set_index("Metric"))

        elif st.session_state.model_trained == 'single':
            st.header("Model Results")
            results = st.session_state.results['single']
            metrics = st.session_state.metrics['single']
            
            display_custom_prediction("Model", results, user_text, show_analysis=True)
            
            st.subheader("Performance Metrics")
            col1, col2 = st.columns(2)
            with col1:
                m_col1, m_col2 = st.columns(2)
                m_col1.metric("Accuracy", f"{metrics['accuracy']:.4f}")
                m_col2.metric("F1 Score", f"{metrics['f1']:.4f}")
                m_col1.metric("Precision", f"{metrics['precision']:.4f}")
                m_col2.metric("Recall", f"{metrics['recall']:.4f}")
                st.write("**Confusion Matrix**")
                st.write(metrics['confusion_matrix'])
            with col2:
                fig, _ = plot_roc_curve(st.session_state.y_test, results['probabilities'])
                if fig:
                    st.pyplot(fig)

    except FileNotFoundError:
        st.error("Default 'Education.csv' not found. Please upload a CSV file to begin.")
    except Exception as e:
        st.error(f"An unexpected error occurred in the application.")
        st.exception(e)

if __name__ == "__main__":
    main()